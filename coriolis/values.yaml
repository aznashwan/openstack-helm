
# List of provider plugins to enable/include:
# TODO: unused yet, everything is hardcoded as enabled.
providers:
  source:
    - aws
    - azure
    - openstack
    - opc
    - oracle_vm
    - vmware_vsphere
  destination:
    - aws
    - azure
    - oci
    - opc
    - oracle_vm
    - openstack
    - scvmm

# Labels used by openstack-helm:
labels:
  api:
    node_selector_key: openstack-control-plane
    node_selector_value: enabled
  conductor:
    node_selector_key: openstack-control-plane
    node_selector_value: enabled
  replica_cron:
    node_selector_key: openstack-control-plane
    node_selector_value: enabled
  worker:
    node_selector_key: openstack-control-plane
    node_selector_value: enabled
  web:
    node_selector_key: openstack-control-plane
    node_selector_value: enabled
  web-proxy:
    node_selector_key: openstack-control-plane
    node_selector_value: enabled
  job:
    node_selector_key: openstack-control-plane
    node_selector_value: enabled

release_group: null

images:
  tags:
     # TODO: check if these images are compatible:
    bootstrap: docker.io/openstackhelm/heat:stein-ubuntu_bionic
    db_init: docker.io/openstackhelm/heat:stein-ubuntu_bionic
    db_drop: docker.io/openstackhelm/heat:stein-ubuntu_bionic
    dep_check: quay.io/airshipit/kubernetes-entrypoint:v1.0.0
    # NOTE: db sync will be performed by the conductor on startup
    rabbit_init: docker.io/rabbitmq:3.7-management
    ks_user: docker.io/openstackhelm/heat:stein-ubuntu_bionic
    ks_service: docker.io/openstackhelm/heat:stein-ubuntu_bionic
    ks_endpoints: docker.io/openstackhelm/heat:stein-ubuntu_bionic
    coriolis_db_sync: registry.cloudbase.it/appliance/coriolis-conductor:latest
    # TODO: find smaller image with `wget` and `tar` installed to use for this:
    coriolis_vixdisklib_init: registry.cloudbase.it/appliance/coriolis-api:latest
    coriolis_api: registry.cloudbase.it/appliance/coriolis-api:latest
    coriolis_conductor: registry.cloudbase.it/appliance/coriolis-conductor:latest
    coriolis_replica_cron: registry.cloudbase.it/appliance/coriolis-replica-cron:latest
    coriolis_worker: registry.cloudbase.it/appliance/coriolis-worker:latest
    coriolis_web: registry.cloudbase.it/appliance/coriolis-web:latest
    coriolis_web_proxy: registry.cloudbase.it/appliance/coriolis-web-proxy:latest
  pull_policy: "IfNotPresent"
  imageCredentials:
    coriolisImagePullSecret: cbslreg
  local_registry:
    active: false
    exclude:
      - dep_check
      - image_repo_sync

pod:
  security_context:
    coriolis_api:
      pod:
        runAsUser: 42424
      container:
        coriolis_api:
          readOnlyRootFilesystem: true
          allowPrivilegeEscalation: false
          runAsUser: 0
    coriolis_conductor:
      pod:
        runAsUser: 42424
      container:
        coriolis_conductor:
          redOnlyRootFilesystem: true
          runAsUser: 0
    coriolis_replica_cron:
      pod:
        runAsUser: 42424
      container:
        coriolis_replica_cron:
          readOnlyRootFilesystem: true
          allowPrivilegeEscalation: false
          runAsUser: 0
    coriolis_worker:
      coriolis_worker_opt_path: /opt/coriolis
      pod:
        runAsUser: 42424
      container:
        # TODO: do init stuff like downloading/setting vixDiskLib
        init_coriolis_vixdisklib:
          runAsUser: 0
          readOnlyRootFilesystem: false
        init_coriolis_conf:
          runAsUser: 0
          readOnlyRootFilesystem: true
        coriolis_worker:
          readOnlyRootFilesystem: true
          allowPrivilegeEscalation: false
          runAsUser: 0
        # TODO:
        # - define check/create/mount external bin dir
    coriolis_web:
      pod:
        runAsUser: 42424
      container:
        # TODO: define:
        init_coriolis_web_conf:
          runAsUser: 0
          readOnlyRootFilesystem: true
        coriolis_web:
          readOnlyRootFilesystem: true
          allowPrivilegeEscalation: false
          runAsUser: 0
    coriolis_web_proxy:
        # TODO: define:
        init_coriolis_web_proxy_conf:
          runAsUser: 0
          readOnlyRootFilesystem: true
        coriolis_web_proxy:
          readOnlyRootFilesystem: true
          allowPrivilegeEscalation: false
          runAsUser: 0

  affinity:
    anti:
      type:
        default: preferredDuringSchedulingIgnoredDuringExecution
      topologyKey:
        default: kubernetes.io/hostname
      weight:
        default: 10

  mounts:
    # NOTE: used to specify any additional desired mounts
    # The standard mounts (ex: /etc and binary files) are handled
    # in the templates automatically:
    coriolis_api:
      init_container: null
      coriolis_api:
        volumeMounts:
        volumes:
    coriolis_conductor:
      init_container: null
      coriolis_conductor:
        volumeMounts:
        volumes:
    coriolis_replica_cron:
      init_container: null
      coriolis_replica_cron:
        volumeMounts:
        volumes:
    coriolis_worker:
      init_container: null
      coriolis_worker:
        volumeMounts:
        volumes:
    coriolis_web:
      init_container: null
      coriolis_web:
        volumeMounts:
        volumes:
    coriolis_web_proxy:
      init_container: null
      coriolis_web_proxy:
        volumeMounts:
        volumes:
    coriolis_db_sync:
      init_container: null
      coriolis_db_sync:
        volumeMounts:
        volumes:

  replicas:
    api: 1
    conductor: 1
    replica_cron: 1
    worker: 1
    web: 1
    web_proxy: 1

  lifecycle:
    upgrades:
      deployments:
        # NOTE: useful when upgrading the chart:
        revision_history: 1
        pod_replacement_strategy: RollingUpdate
        rolling_update:
          max_unavailable: 1
          max_surge: 3
    disruption_budget:
      api:
        min_available: 0
    termination_grace_period:
      api:
        timeout: 30

  resources:
    # TODO: try enabling to see how it works:
    enabled: false
    api:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "1024Mi"
        cpu: "2000m"
    conductor:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "1024Mi"
        cpu: "2000m"
    replica_cron:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "1024Mi"
        cpu: "2000m"
    worker:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "1024Mi"
        cpu: "2000m"
    web:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "1024Mi"
        cpu: "2000m"
    web-proxy:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "1024Mi"
        cpu: "2000m"
    jobs:
      # TODO: check jobs list:
      bootstrap:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      clean:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      create_internal_tenant:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      db_init:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      db_sync:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      db_drop:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      ks_endpoints:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      ks_service:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      ks_user:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"
      rabbit_init:
        requests:
          memory: "128Mi"
          cpu: "100m"
        limits:
          memory: "1024Mi"
          cpu: "2000m"

bootstrap:
  enabled: true
  ks_user: admin
  # TODO: add functionalities such as pre-creating endpoints and such here

network:
  api:
    ingress:
      public: true
      classes:
        namespace: "nginx"
        cluster: "nginx-cluster"
      annotations:
        nginx.ingress.kubernetes.io/rewrite-target: /
    external_policy_local: false
    node_port:
      enabled: false
      port: 30667

conf:
  paste:
    composite:coriolis-api:
      use: call:coriolis.api:root_app_factory
      /v1: coriolis-api-v1
    pipeline:coriolis-api-v1:
      pipeline: request_id faultwrap authtoken keystonecontext apiv1
    app:apiv1:
      paste.app_factory: coriolis.api.v1.router:APIRouter.factory
    filter:authtoken:
      paste.filter_factory: keystonemiddleware.auth_token:filter_factory
    filter:faultwrap:
      paste.filter_factory: coriolis.api.middleware.fault:FaultWrapper.factory
    filter:keystonecontext:
      paste.filter_factory: coriolis.api.middleware.auth:CoriolisKeystoneContext.factory
    filter:request_id:
      paste.filter_factory: oslo_middleware.request_id:RequestId.factory
  policy:
    admin: "role:admin"
    admin_or_owner: "rule:admin or project_id:%(project_id)s"

    migration:providers:list: "rule:admin_or_owner"

    migration:endpoints:create: "rule:admin_or_owner"
    migration:endpoints:list: "rule:admin_or_owner"
    migration:endpoints:show: "rule:admin_or_owner"
    migration:endpoints:update: "rule:admin_or_owner"
    migration:endpoints:delete: "rule:admin_or_owner"
    migration:endpoints:validate_connection: "rule:admin_or_owner"
    migration:endpoints:list_instances: "rule:admin_or_owner"
    migration:endpoints:get_instance: "rule:admin_or_owner"
    migration:endpoints:list_networks: "rule:admin_or_owner"
    migration:endpoints:list_source_options: "rule:admin_or_owner"
    migration:endpoints:list_destination_options: "rule:admin_or_owner"

    migration:migrations:create: "rule:admin_or_owner"
    migration:migrations:list: "rule:admin_or_owner"
    migration:migrations:show: "rule:admin_or_owner"
    migration:migrations:show_execution: "rule:admin_or_owner"
    migration:migrations:cancel: "rule:admin_or_owner"
    migration:migrations:delete: "rule:admin_or_owner"

    migration:replicas:create: "rule:admin_or_owner"
    migration:replicas:list: "rule:admin_or_owner"
    migration:replicas:show: "rule:admin_or_owner"
    migration:replicas:show_executions: "rule:admin_or_owner"
    migration:replicas:delete_disks: "rule:admin_or_owner"
    migration:replicas:delete: "rule:admin_or_owner"

    migration:replica_executions:create: "rule:admin_or_owner"
    migration:replica_executions:list: "rule:admin_or_owner"
    migration:replica_executions:show: "rule:admin_or_owner"
    migration:replica_executions:cancel: "rule:admin_or_owner"
    migration:replica_executions:delete: "rule:admin_or_owner"

    migration:replica_schedules:create: "rule:admin_or_owner"
    migration:replica_schedules:list: "rule:admin_or_owner"
    migration:replica_schedules:show: "rule:admin_or_owner"
    migration:replica_schedules:update: "rule:admin_or_owner"
    migration:replica_schedules:delete: "rule:admin_or_owner"
  coriolis:
    DEFAULT:
      # NOTE: will be set by openstack-helm:
      providers:
      transport_url:
      messaging_transport_url:
      default_messaging_timeout: 60

      debug: true
      # TODO: abstract logging better:
      log_dir: /var/log/coriolis/
      log_config_append: /etc/coriolis/logging.conf
      logging_exception_prefix: "%(asctime)s.%(msecs)03d TRACE %(name)s %(instance)s"
      logging_debug_format_suffix: "from (pid=%(process)d) %(funcName)s %(pathname)s:%(lineno)d"
      logging_default_format_string: "%(asctime)s.%(msecs)03d %(levelname)s %(name)s [-] %(instance)s%(message)s"
      logging_context_format_string: "%(asctime)s.%(msecs)03d %(levelname)s %(name)s [%(request_id)s] %(instance)s%(message)s"

      include_task_info_in_migrations_api: false
      include_task_info_in_replicas_api: false
      include_task_info_in_replica_executions_api: false

      caching: true
      cache_time: 86400
      use_syslog: yes
      syslog_log_facility: LOG_LOCAL0

      internal_project_name: internal_coriolis
      internal_user_name: internal_coriolis

    cache:
      # TODO: enable caching
      enabled: false
      backend: dogpile.cache.memcached
      backend_argument: url:localhost:11211

    database:
      # NOTE: to be set by openstack-helm:
      connection:
      max_retries: -1

    keystone_authtoken:
      # NOTE: all the other required params will be set in the configmap-etc job:
      auth_version: v3
      auth_type: password
      memcache_security_strategy: ENCRYPT
    oslo_concurrency:
      lock_path: "/var/lib/coriolis/tmp"
    oslo_messaging_notifications:
      driver: messagingv2
    trustee:
      # TODO:
    keystone:
      # TODO:
    oslo_policy:
      # TODO: avoid hardcoding this:
      policy_file = /etc/coriolis/policy.json
    conductor:
      debug_os_morphing_errors: false
      conductor_rpc_timeout: 60
    worker:
      worker_rpc_timeout: 60

  providers:
    provider_conf_file_mount_dir: "/etc/coriolis/providers"
    provider_conf_file_name_format: "%s-migration-provider.conf"
    aws:
      conf: |
        [aws_migration_provider]
        # The OS migration image map to use. This mapping will be used to spin up
        # the disk export worker needed to replicate the export disks and OS morphing
        # worker needed to morph the OS being migrated.
        # migr_image_map = linux: ami-50946030, windows: ami-50946031
        
        # A map similar with migr_image_map, where we specify the username for each OS
        # image. On EC2, various linux distributions, have different usernames.
        # Default is 'ubuntu' for Linux and 'coriolis' for Windows.
        # migr_image_username_map = linux: ubuntu, windows: coriolis
        
        # Name of the instance type to be used for the disk copy workers.
        # Default is t2.medium.
        worker_instance_type = t2.medium
        
        # Indicate whether or not to shut the VM down during the
        # migration process in order to ensure data consitency.
        shutdown_migrated_instance = False
        
        # Default instance type used for final migrated instances.
        # instance_type = t2.medium
        
        # Whether or not to attempt to retain the IP addresses the VM had on the
        # source. This requires that the mapped subnets on AWS include the respective
        # IP address(es) in their ranges.
        # Default is False.
        retain_source_ip = false
        
        # The storage type to default to for the disks of Migrated VMs if an explicit
        # storage mapping was not supplied during the Replica/Migration creation.
        # Default is 'standard'
        default_storage_type = standard
        
        # Default AD to use when creating resources for Migrated/Replicated VMs.
        # Must be in the same region as specified within the connection info.
        # availability_zone = az1

    azure:
      conf: |
        [azure_migration_provider]
        ### Export parameters:
        
        # The Azure location to use for any temporary export resources.
        # Under most circumstances, this should be the same as the location of the VM
        # being Migrated/Replicated which should be specified in the source-env.
        export_location = westus
        
        # Default Migration worker VM size. This will be used if no worker_size is set in
        # the source_environment. If none is provided, Coriolis will list all available
        # flavors on Azure and select the smallest available one that can accommodate
        # the number of disks. If not given, Coriolis will select the first worker
        # size available which can accomodate attaching all the disks for the
        # particular VM being migrated/replicated.
        export_worker_size = Standard_D1
        
        # Params of the Azure image to use for the temporary VM:
        export_worker_image = version: latest, publisher: Canonical, offer: UbuntuServer, sku: 18.04-LTS
        
        # For use with AzureStack, the following ARM Provider API versions must be
        # configured to API versions supported by the AzureStack. If unset, they will
        # default to the latest API versions which are supported by the ARM client
        # libraries bundled with Coriolis.
        # export_arm_resource_api_version = 2016-09-01
        # export_arm_compute_api_version = 2016-03-30
        # export_arm_network_api_version = 2015-06-15
        # export_blob_storage_api_version = 2015-04-05
        
        ### Import parameters:
        
        # The Azure location to which to migrate. The location may also be set for each
        # migration/replica in turn using the destination-environment paramter.
        migr_location = westus
        
        # Name of the Azure storage account to create VHD Page blobs on if
        # "disk_storage_backing_type" is set to "blob_storage"
        # storage_account_name = storageaccount
        
        # Name of the Azure storage container to create VHD page blobs on if
        # "disk_storage_backing_type" is set to 'blob_storage'
        # storage_container_name = coriolis
        
        # The type of storage backing to use for the disks of VMs migrated to Azure.
        # If set to 'blob_storage', disks will be transferred as VHD Page Blobs inside
        # a pre-existing Blob Storage Account inside the resource group being migrated
        # to. If set to 'managed_disks', disks will be transferred as Managed Disks
        # within the resource group being migrated to. If not set, Coriolis will query
        # the available Compute API versions to determine if Managed Disks are
        # available (2017-03-30 onwards), else it defaults to using Blob Storage.
        disk_storage_backing_type = managed_disks
        
        # Default migrated instance size. This will be used if no vm_size is set in
        # the destination-environment. Note: migration will fail if migrated instance
        # size cannot accommodate the number of disks than can be migrated.
        # If not set, the smallest available size that can accommodate the number of
        # disks will be selected for the VM.
        # default_vm_size = Standard_A1
        
        # The images to use for temporary worker VMs:
        linux_migr_image = version: latest, publisher: Canonical, offer: UbuntuServer, sku: 18.04-LTS
        windows_migr_image = version: latest, publisher: MicrosoftWindowsServer, offer: WindowsServer, sku: 2016-Datacenter-Server-Core
        
        # Default migration worker size. This will be used if no worker_size is set in
        # the target_environment. If none is provided, Coriolis will list all available
        # flavors on Azure and select the smallest available one that can accommodate
        # the number of disks. If not given, Coriolis will select the first worker
        # size available which can accomodate attaching all the disks for the
        # particular VM being migrated/replicated.
        default_worker_size = Standard_D1
        
        # Whether or not to set any IP addresses read from source VM NIC configurations
        # to destination NICs on Azure. This requires that the network/subnet mappings
        # in the 'network_map' are to networks/subnets on Azure which include the range
        # of the source networks. Default is True
        preserve_nic_ips = True
        
        # For use with AzureStack, the following ARM Provider API versions must be
        # configured to API versions supported by the AzureStack. If unset, they will
        # default to the latest API versions which are supported by the ARM client
        # libraries bundled with Coriolis.
        # arm_resource_api_version = 2016-09-01
        # arm_compute_api_version = 2016-03-30
        # arm_network_api_version = 2015-06-15
        # blob_storage_api_version = 2015-04-05
        # arm_storage_api_version = 2018-07-01
        
        # Cloudbase-init Zip URLs for each supported Windows architecture:
        cloudbaseinit_x86_url = https://www.cloudbase.it/downloads/CloudbaseInitSetup_x86.zip
        cloudbaseinit_x64_url = https://www.cloudbase.it/downloads/CloudbaseInitSetup_x64.zip

    oci:
      conf: |
        [oci_migration_provider]
        # You need to configure this to match your situation
        # there is no sensible default that can work globally. The images
        # are specific to the region you are in. The bellow ones are from
        # us-phoenix-1
        # migr_image_map = linux: ocid1.image.oc1.phx.aaaaaaaatel6prh7kwlb77zp6m4v523pegz3fh2j5ulq2lwtmhionpy37wkq, windows: ocid1.image.oc1.phx.aaaaaaaam3rbuegojxzdomvbp6qa2fb45ex24ng7c5ykawuqmomerfblveaq
        
        # Whether to use paravirtualized mode by default for
        # instances that use BIOS.
        default_to_pv_mode = true
        
        # Default shape name used for final instances
        shape_name = VM.Standard1.2
        
        # Default shape name used for worker instances
        # during migrations.
        migr_shape_name = VM.Standard1.2
        
        # Default network name used for worker instances
        # during migrations.
        # migr_subnet_id =
        
        # Default compartment in which the migration should take place.
        # compartment =
        
        # Whether or not to set Public IPs on the Migrated/Replicated VMs:
        set_public_ip = True
        
        # Default attach type to be used when attaching secondary
        # disks to the migrated instance. Accepted values:
        # 'iscsi', 'emulated', 'paravirtualized'
        default_attach_type = iscsi
        
        # Windows VMs that are migrated to OCI need virtio drivers.
        # The steps to downloading GA drivers can be found at:
        # https://blogs.oracle.com/linux/announcing-oracle-virtio-drivers-113-for-microsoft-windows
        # The downloaded ZIP has to be self-hosted. Pass the ZIP URL to the option below:
        # windows_virtio_zip_url = https://example.com/virtio-1.1.3.zip

    opc:
      conf: |
        [opc_migration_provider]
        # Linux and Windows images for the temporary Worker VMs:
        # migr_image_name_map = linux: /oracle/public/OL_7.5_UEKR4_x86_64_MIGRATION, windows: /Compute-a488347/gsamfira@cloudbasesolutions.com/Windows_2012_R2
        migr_image_map = linux: /oracle/public/OL_7.5_UEKR4_x86_64_MIGRATION
        
        # The shape to use for the migration Worker VMs:
        migr_shape_name = oc3
        
        # Default volume pool to use. Oracle exposes 2 pools: "default" and "latency".
        # The default pool leverages rotary disks, while the latency disks are backed
        # by SSD and low latency storage subsystem. The latency pool is suitable for
        # databases.
        default_volume_pool = /oracle/public/storage/default
        
        # Default network name used for worker instances
        # during migrations.
        # migr_network_name = /Compute-a514847/default
        
        # URL of a zip file with the Windows PV drivers:
        windows_pv_drivers_url = https://cloudbase.it/downloads/app/ovm/ovm_win_pv_drivers_all_323_bare_root.zip
        
        # Image used for the export worker
        export_image_name = /oracle/public/OL_7.2_UEKR4_x86_64
        
        # Shape used for export worker
        export_shape_name = oc3
        
        # Exporting an instance may require an extra disk to be attached, to accomodate the size of the
        # largest disks that belong to the exported instance. Make sure to allocate enough space here.
        export_root_disk_size = 150
        
        # Default shape name used for migrated instances.
        shape_name = oc3
        
        # Admin username for the export_image_name. Default username is 'opc'.
        # export_img_username =

    openstack:
      conf: |
        [openstack_migration_provider]
        ### Export parameters:
        
        # Integer version of the Image API to use for any interactions with Glance,
        # supported versions being 1 and 2.
        # This value may be overriden on a per-cloud basis with the 'glance_api_version'
        # parameter in the endpoint connection info.
        # Default is 1
        glance_api_version = 1
        
        # Whether to skip certificate verification if the Swift proxy is HTTPS-only
        # with self-signed certificates.
        # This value may be overriden on a per-cloud basis with the
        # 'allow_untrusted_swift' parameter in the connection info.
        # Default is False
        allow_untrusted_swift = False
        
        # Custom mapping between the 'os_type' or 'os_distro' of Glance images on the
        # source OpenStack. Mapping values must be one of the supported OS types in
        # Coriolis. ('linux', 'windows', or 'solaris')
        # custom_os_type_map =
        
        # What Cinder implementation to leverage during replica exports.
        # If set to 'swift_backups', Coriolis will create Cinder backups for each
        # replicated VM disk and fetch the backup objects from Swift.
        # If set to 'ceph_backups', Coriolis will create Cinder backups and connect
        # to the source OpenStack's Ceph via RADOS to retrieve the chunks of each disk.
        # If set to 'ceph_snapshots', Coriolis will create Cinder snapshots for each disk
        # and fetch the diff chunks from the source OpenStack's Ceph.
        # Both the 'ceph_backups' and 'ceph_snapshots' options require that the
        # coriolis-worker component have network access to the source OpenStack's Ceph
        # through librbd/librados.
        # If set to 'coriolis_backups', Coriolis will create a temporary VM on the source
        # and use its in-built Replication engine through it to diff/export the disks.
        # Only the 'coriolis_backups' mechanism works for VMs which were booted from a
        # Glance image, and all other options only work on Cinder-based VMs.
        # Default is 'swift_backups'
        replica_export_mechanism = swift_backups
        
        ## Options for 'replica_export_mechanism = swift_backups'
        
        # The name of the Swift container in which to store the Cinder backups.
        # If pre-existing, the container will be reused. If it doesn't exist, a new one
        # will be created.
        # Default is 'coriolis'
        volume_backups_container = coriolis
        
        ## Options for 'replica_export_mechanism = ceph_backups/snapshots'
        
        # Integer timeout (seconds) when connecting to Ceph.
        # Default is 30
        cinder_ceph_timeout = 30
        
        # String name of algorith in Python's hashlib to use for hashing disk chunks to
        # compare them. If unset, the diff will be compared directly instead of hashing.
        # cinder_ceph_backups_diff_hash_algorithm = None
        
        ## Options for 'replica_export_mechanism = coriolis_backups'
        
        # Name or ID of a pre-existing Glance Ubuntu 18.04 image on the source OpenStack
        # to be used for the temporary export worker VM on the source. The image must be
        # available to the project/tenant provided in the Coriolis Endpoint. The images
        # must have cloud-init installed and configured for first boot.
        # export_image = ubuntu-18.04-coriolis-worker
        
        # Name of pre-existing Cinder volume type on the source to use for the temporary
        # volumes Coriolis will be creating for exporting instance Glance images. In the
        # case of secondary volumes, the storage type already used by the individual
        # volumes will be used to avoid a cross-backend volume transfer while the clone
        # is occurring. If unset, the default Cinder volume type will be used.
        # export_interim_volume_type = 
        
        # Whether or not to download the worker image to a Cinder volume and boot the
        # worker from that. The root disk size" on the configured 'export_flavor_name'
        # will be used for the worker VM volume.
        export_worker_boot_from_volume = false
        
        # Name of the precreated Cinder volume type to use when creating temporary worker volumes.
        # This option is only effective if "export_worker_boot_from_volume" is set.
        # export_worker_volume_type = cinder-volume-type
        
        # The integer size (in GBs) of the Cinder volume to boot temporary worker VMs from.
        # This option is only effective if "export_worker_boot_from_volume" is set. If not set,
        # Coriolis will use the disk size set the selected "export_flavor_name".
        # export_worker_volume_size = 100
        
        # Name of the flavor to use for the temporary disk export VM.
        # The flavor must be compatible with the selected 'export_image'.
        # export_flavor_name = m1.small
        
        # Name or ID of an existing Neutron network on the source OpenStack where to attach
        # the Neutron port of the temporary disk export worker VMs. The selected network
        # must either be in the same project/tenant as set in the Coriolis endpoint or
        # be a shared/external network and thus available to said project. If
        # export_worker_use_fip is set to 'false', the Coriolis installation must be
        # able to route to addresses allocated from this network."
        # export_network = external-network-name
        
        # Whether or not to allocate floating IPs from 'export_fip_pool_name' for the
        # temporary disk export worker VMs.
        # Default is true
        # export_worker_use_fip = true
        
        # Name of an external Neutron network from which to allocate floating IPs for the
        # temporary worker VMs Coriolis boots up during the disk export process. The
        # Coriolis installation must be able to route to addresses allocated from this network.
        # export_fip_pool_name = external
        
        # Whether or not to use config drive to send metadata to the temporary disk disk
        # export worker VMs in case Neutron metadata is not available. Default is false.
        # export_worker_use_config_drive = false
        
        ### Import parameters:
        
        # If migrating/replicating a heterogeneous workload with both Linux and
        # Windows instances, separate image names or IDs will need to be provided
        # for the temporary OSMorphing workers for each OS type.
        # The 'linux' image will be used for the disk copy worker during the replica
        # execution process as well
        # Only accepted keys are 'linux' and 'windows'
        # migr_image_map = linux: 283bdce4-112c-4758-b06a-7e5985a9cd15, windows: Windows Server 2012 R2 Std Eval
        
        # Boolean flag indicating whether or not to list all Neutron networks.
        # By default, Coriolis will only list the networks which are in the
        # same tenant as the one set in the Coriolis endpoint in use.
        list_all_destination_networks = False
        
        # Whether or not unallocated blocks on Cinder target volumes contain
        # zeros. This is controlled by the "volume_clear" Cinder configuration
        # options. (boolean value)
        volumes_are_zeroed = True
        
        # Default hypervisor type. (string value)
        # The type of hypervisor the destination OpenStack features,
        # supported options being "hyperv", "qemu" or "kvm"
        # Default is None, meaning no extra hypervisor-specific actions will be taken
        # during the OSMorphing process
        hypervisor_type = kvm
        
        # Whether or not to download the worker image to a Cinder volume and boot the
        # worker from that. The root disk size on the configured 'migr_flavor_name'
        # will be used for the worker VM volume, 'migr_flavor_name' will be used for
        # the worker VM volume unless explicitly specified using 'migr_worker_volume_size'.
        migr_worker_boot_from_volume = False
        
        # The integer size (in GBs) of the Cinder volume to boot temporary worker VMs
        # from. This option is only effective if "migr_worker_boot_from_volume" is set.
        # If not set, Coriolis will use the disk size set the selected "migr_flavor_name".
        # migr_worker_volume_size =
        
        # Name of the precreated Cinder volume type to use when creating temporary worker volumes.
        # This option is only effective if "migr_worker_boot_from_volume" is set.
        # migr_worker_volume_type =
        
        # Name of the Cinder volume type to be used for
        # volumes with unspecified storage backing option from the
        # source or which could not be mapped in the "storage_map".
        # Default is "" (Cinder scheduler will
        # use default volume type)
        # default_cinder_volume_type =
        
        # Whether or not to set the Cinder volumes as ephemeral (to be deleted when
        # the VM is deleted) on the newly migrated/replicated instance on the
        # destination OpenStack. Default is False.
        delete_disks_on_vm_termination = True
        
        # List of names or IDs of pre-existing security groups on the destination
        # OpenStack to be applied to the migrated/replicated instance.
        # Note that the below set will be joined with the list specified via the
        # 'security_groups' parameter from the set of destination environment parameters
        # (list value)
        # default_security_groups = secgroup1, secgroup2
        
        # Name or ID of an existing Neutron network on the destination
        # OpenStack where to attach the Neutron port of the temporary disk
        # copy/OSMorphing worker VMs. (string value)
        # migr_network = private
        
        # Whether or not to allocate floating IPs from the "migr_fip_pool_name"
        # for the temporary disk copy/OSMorphing worker VMs.
        # Default is True
        migr_worker_use_fip = True
        
        # Name of an existing Neutron network on the destination OpenStack which
        # serves as external and may have floating IPs allocated from it.
        # These floating IPs will be associated to the temporary disk copy/OSMorphing
        # worker VMs.
        # Default is "public"
        # migr_fip_pool_name = public
        
        # Name of an existing Nova flavor which to boot the temporary
        # disk copy/OSMorphing worker VMs as.
        # Default is "m1.small"
        # (string value)
        # migr_flavor_name = m1.small
        
        # Whether or not to use config drive to send metadata to the temporary
        # disk copy/OSMorphing worker VMs in case Neutron metadata is not available
        # Default is False
        migr_worker_use_config_drive = False
        
        # Whether or not to reconfigure the internal network settings of each
        # interface of the instance being migrated/replicated during the OSMorphing
        # process to have the VM perform DHCP on first boot inside the new environment.
        # Default is True
        set_dhcp = True
        
        # If set to "reuse_ports", Coriolis will reuse any Neutron ports found with
        # the MAC addresses needed for the new VM. If set to "keep_mac", Coriolis
        # will delete any ports with the MAC addresses the VM needs and recreate them
        # with the same MAC address. If set to "create_new_ports", Coriolis will always
        # create new ports with new MAC addresses.
        port_reuse_policy = keep_mac
        
        # Dictionary with arbitrary key-value pairs to set as tags on the
        # migrated/replicated VMs.
        # instance_tags =
        
        # Whether or not to preserve the fixed IPs of the migrated instances' Neutron
        # ports. When this is set to 'True', the target Openstack cloud will attempt
        # to create ports containing fixed IPs collected from the source VM. A subnet
        # with the right CIDR is required in the mapped network for the VM's first NIC.
        preserve_fixed_ips = False
        
        # Whether or not to attach a floating IP to the already migrated VM. This
        # option must be set to 'True' if the migrated VM is intended to have a public
        # IP attached. The floating IP will be allocated from the 'floating_ip_pool'
        # option.
        use_floating_ip = False
        
        # Name of the floating IP pool to be used for the creation and attachment of
        # floating IPs to the migrated VMs.
        # floating_ip_pool = public
        
        # Name or ID of the Nove Server Group to use when recreating the final VMs on Openstack
        # server_group =
        
        # Parameters realted to the OSMorphing process for Windows instances:
        windows_virtio_iso_url = https://fedorapeople.org/groups/virt/virtio-win/direct-downloads/stable-virtio/virtio-win.iso
        cloudbaseinit_x64_url = https://www.cloudbase.it/downloads/CloudbaseInitSetup_x64.zip
        cloudbaseinit_x86_url = https://www.cloudbase.it/downloads/CloudbaseInitSetup_x86.zip

    oracle_vm:
      conf: |
        [oracle_vm_migration_provider]
        # If migrating/replicating a heterogeneous workload with both Linux and
        # Windows instances, separate template names or IDs will need to be provided
        # # for the temporary OSMorphing workers for each OS type.
        # The 'linux' image will be used for the disk copy worker during the replica
        # execution process as well
        # Only accepted keys are 'linux' and 'windows'
        # migr_template_map = linux: OracleLinux7_template, windows: WS2016_template
        
        # Default migration template guest OS username.
        # migr_template_username_map = linux: root, windows: Administrator
        
        # Default migration template guest OS password.
        # migr_template_password_map = linux: Passw0rd, windows: Passw0rd
        
        # Default network name used for worker instances
        # during migrations.
        # migr_network_name = management
        
        # Default server pool name.
        # server_pool_name = pool1
        
        # Default repository name.
        # repository_name = repo1
        
        # Whether or not to just create VMs but skip powering them on.
        leave_migrated_vm_off = False
        
        # XEN VM domain type.
        # can be 'XEN_PVM', 'XEN_HVM', 'XEN_HVM_PV_DRIVERS', 'LDOMS_PVM', 'UNKNOWN'
        vm_domain_type = XEN_HVM_PV_DRIVERS
        
        # virtual_disk_clone_type can be: THIN_CLONE, SPARSE_COPY, NON_SPARSE_COPY
        virtual_disk_clone_type = THIN_CLONE
        
        # Location of the Cloudbase-Init ZIP for each supported architecture.
        # Cloudbase-Init is installed in Windows VMs for some of the auxiliary
        # functionality it offers to the migrated VMs (most notably, startup scripts)
        cloudbaseinit_x86_url = https://www.cloudbase.it/downloads/CloudbaseInitSetup_x86.zip
        cloudbaseinit_x64_url = https://www.cloudbase.it/downloads/CloudbaseInitSetup_x64.zip
        
        windows_pv_drivers_url = https://cloudbase.it/downloads/app/ovm/ovm_win_pv_drivers_all_323_bare_root.zip
        
        # For export we may need to use a different template than the linux one
        # specified in the template map, though in most cases the same one should work.
        # We specify that here (either by name or ID) with its credentials
        # export_template = OracleLinux7_template
        # export_template_username = root
        # export_template_password = Passw0rd

    scvmm:
      conf: |
        [scvmm_migration_provider]
        ### Export parameters:
        
        # Attempt application consistent snapshots, and if that fails, fallback to
        # crash consistent snapshots.
        fallback_to_crash_consistent_snapshots = True
        
        # Verify the SSL certificate of the RTC service.
        # Set this to False if using a self signed certificate.
        verify_rct_server = False
        
        ### Import parameters:
        
        # Default vm templates used for worker instances during migrations
        # migr_template_name_map =
        
        # Default migration template guest OS username.
        # migr_template_username_map =
        
        # Cloud name, used for scheduling VMs.
        # cloud_name =

    vmware_vsphere:
      vix_disklib_tar_url: https://cloudbase.it/downloads/VMware-vix-disklib-6.0.3-4888596.x86_64.tar.gz
      # NOTE: make sure this dir is the same as the
      # 'vixdisklib_library_directory' in the conf below!
      vix_disklib_mount_dir: /opt/coriolis/vmware-vix-disklib
      conf: |
        [vmware_vsphere_migration_provider]
        # Required in order to set the compatibility of the vixDiskLib release
        # Coriolis will use during replication/CBT-based migration from VMWare.
        # This value should ideally match the ESXi/vCenter hosts versions.
        vixdisklib_compatibility_version = 6.0
        
        # The path where the SOs for vixDiskLib are located:
        # NOTE: for Ubuntu worker VMs, we must prevent the libglib shipped with
        # vixDiskLib to override the system's libglib by prepending the system's
        # library paths (for 16.04 and 18.04, respectively):
        vixdisklib_library_directory = /lib/x86_64-linux-gnu:/usr/lib/x86_64-linux-gnu:/opt/coriolis/vmware-vix-disklib
        
        # Whether or not Coriolis should attempt to automatically enable CBT on
        # the VM before Replication.
        # This requires that the VM have no pre-existing snapshots.
        automatically_enable_cbt = False

  logging:
    loggers:
      keys:
        - root
        - coriolis
    handlers:
      keys:
        - stdout
        - stderr
        - "null"
    formatters:
      keys:
        - context
        - default
    logger_root:
      level: WARNING
      handlers: stdout
    logger_coriolis:
      level: DEBUG
      handlers:
        - stdout
      qualname: coriolis
    logger_amqp:
      level: WARNING
      handlers: stderr
      qualname: amqp
    logger_amqplib:
      level: WARNING
      handlers: stderr
      qualname: amqplib
    logger_eventletwsgi:
      level: WARNING
      handlers: stderr
      qualname: eventlet.wsgi.server
    logger_sqlalchemy:
      level: WARNING
      handlers: stderr
      qualname: sqlalchemy
    logger_boto:
      level: WARNING
      handlers: stderr
      qualname: boto
    # TODO: research logging options for coriolis-logger:
    handler_null:
      class: logging.NullHandler
      formatter: default
      args: ()
    handler_stdout:
      class: StreamHandler
      args: (sys.stdout,)
      formatter: context
    handler_stderr:
      class: StreamHandler
      args: (sys.stderr,)
      formatter: context
    formatter_context:
      class: oslo_log.formatters.ContextFormatter
      datefmt: "%Y-%m-%d %H:%M:%S"
    formatter_default:
      format: "%(message)s"
      datefmt: "%Y-%m-%d %H:%M:%S"
  rabbitmq:
    #NOTE(rk760n): adding rmq policy to mirror messages from notification queues and set expiration time for the ones
    policies:
      # TODO: check if these are necessary
      - vhost: "coriolis"
        name: "ha_ttl_coriolis"
        definition:
          #mirror messges to other nodes in rmq cluster
          ha-mode: "all"
          ha-sync-mode: "automatic"
          #70s
          message-ttl: 70000
        priority: 0
        apply-to: all
        pattern: '^(?!(amq\.|reply_)).*'
  # TODO: see what these are for:
  #resource_filters:
  #  volume:
  #    - name
  #    - status
  #    - metadata
  #    - bootable
  #    - migration_status
  #    - availability_zone
  #    - group_id
  #  backup:
  #    - name
  #    - status
  #    - volume_id
  #  snapshot:
  #    - name
  #    - status
  #    - volume_id
  #    - metadata
  #    - availability_zone

dependencies:
  dynamic:
    common:
      local_image_registry:
        jobs:
          - coriolis-image-repo-sync
        services:
          - endpoint: node
            service: local_image_registry
  static:
    api:
      jobs:
        - coriolis-db-sync
        - coriolis-ks-user
        - coriolis-ks-endpoints
        - coriolis-rabbit-init
      services:
        - endpoint: internal
          service: oslo_db
        # NOTE: required for auth:
        - endpoint: internal
          service: identity
    conductor:
      jobs:
        - coriolis-db-sync
        - coriolis-ks-user
        - coriolis-ks-endpoints
        - coriolis-rabbit-init
      services:
        - endpoint: internal
          service: oslo_db
        # NOTE: required for auth and checks:
        - endpoint: internal
          service: identity
    replica_cron:
      jobs:
        - coriolis-db-sync
        - coriolis-ks-user
        - coriolis-ks-endpoints
        - coriolis-rabbit-init
      services:
        - endpoint: internal
          service: oslo_db
        # TODO: check if required:
        - endpoint: internal
          service: identity
    worker:
      jobs:
        - coriolis-db-sync
        - coriolis-ks-user
        - coriolis-ks-endpoints
        - coriolis-rabbit-init
      services:
        - endpoint: internal
          service: oslo_db
        # NOTE: required for fetching Barbican secrets:
        - endpoint: internal
          service: identity
    bootstrap:
      services:
        - endpoint: internal
          service: identity
        - endpoint: internal
          service: volume
      pod:
        - requireSameNode: false
          labels:
            # NOTE: see what this reference points to:
            application: coriolis
            component: volume
    clean:
      jobs: null
    db_drop:
      services:
        - endpoint: internal
          service: oslo_db
    db_init:
      services:
        - endpoint: internal
          service: oslo_db
    db_sync:
      jobs:
        # TODO: define:
        - coriolis-db-init
      services:
        - endpoint: internal
          service: oslo_db
    ks_endpoints:
      jobs:
        # TODO: define:
        - coriolis-ks-service
      services:
        - endpoint: internal
          service: identity
    ks_service:
      services:
        - endpoint: internal
          service: identity
    ks_user:
      services:
        - endpoint: internal
          service: identity
    rabbit_init:
      services:
        - service: oslo_messaging
          endpoint: internal
    # TODO: check if required:
    image_repo_sync:
      services:
        - endpoint: internal
          service: local_image_registry
    create_internal_tenant:
      services:
        - endpoint: internal
          service: identity

# Names of secrets used by bootstrap and environmental checks:
secrets:
  identity:
    admin: coriolis-keystone-admin
    coriolis: coriolis-keystone-user
  oslo_db:
    admin: coriolis-db-admin
    coriolis: coriolis-db-user
  oslo_messaging:
    admin: coriolis-rabbitmq-admin
    coriolis: coriolis-rabbitmq-user
  tls:
    migration:
      api:
        public: coriolis-tls-public

# We use a different layout of the endpoints here to account for versioning
# this swaps the service name and type, and should be rolled out to other
# services.
endpoints:

  # TODO: add web-proxy?

  cluster_domain_suffix: cluster.local

  local_image_registry:
    name: docker-registry
    namespace: docker-registry
    hosts:
      default: localhost
      internal: docker-registry
      node: localhost
    host_fqdn_override:
      default: null
    port:
      registry:
        # TODO: add registry.cloudbase.it port too:
        node: 5000

  identity:
    name: keystone
    auth:
      admin:
        region_name: RegionOne
        username: admin
        password: password
        project_name: admin
        user_domain_name: default
        project_domain_name: default
      coriolis:
        role: admin
        region_name: RegionOne
        username: coriolis
        password: password
        project_name: service
        user_domain_name: service
        project_domain_name: service
    hosts:
      default: keystone
      internal: keystone-api
    host_fqdn_override:
      default: null
    path:
      default: /v3
    scheme:
      default: http
    port:
      api:
        default: 80
        internal: 5000
  migration:
    name: coriolis
    hosts:
      default: coriolis-api
      public: coriolis
    host_fqdn_override:
      default: null
      # NOTE(portdirect): this chart supports TLS for fqdn over-ridden public
      # endpoints using the following format:
      # public:
      #   host: null
      #   tls:
      #     crt: null
      #     key: null
    path:
      default: '/v1/%(tenant_id)s'
    scheme:
      default: 'http'
    port:
      api:
        default: 7667
        public: 80
  oslo_db:
    auth:
      # TODO: check where these are set/used and use them:
      admin:
        username: root
        password: password
      coriolis:
        username: coriolis
        password: password
    hosts:
      default: mariadb
    host_fqdn_override:
      default: null
    path: /coriolis
    scheme: mysql+pymysql
    port:
      mysql:
        default: 3306
  oslo_messaging:
    auth:
      # TODO: check where these are set/used and use them:
      admin:
        username: rabbitmq
        password: password
      coriolis:
        username: coriolis
        password: password
    statefulset:
      replicas: 2
      name: rabbitmq-rabbitmq
    hosts:
      default: rabbitmq
    host_fqdn_override:
      default: null
    path: /coriolis
    scheme: rabbit
    port:
      amqp:
        default: 5672
      http:
        default: 15672
  # TODO: see how/where to set these:
  oslo_cache:
    auth:
      # NOTE(portdirect): this is used to define the value for keystone
      # authtoken cache encryption key, if not set it will be populated
      # automatically with a random value, but to take advantage of
      # this feature all services should be set to use the same key,
      # and memcache service.
      memcache_secret_key: null
    hosts:
      default: memcached
    host_fqdn_override:
      default: null
    port:
      memcache:
        default: 11211
  kube_dns:
    namespace: kube-system
    name: kubernetes-dns
    hosts:
      default: kube-dns
    host_fqdn_override:
      default: null
    path:
      default: null
    scheme: http
    port:
      dns:
        default: 53
        protocol: UDP
  ingress:
    namespace: null
    name: ingress
    hosts:
      default: ingress
    port:
      ingress:
        default: 80

network_policy:
  coriolis:
    ingress:
      - {}
    egress:
      - {}

manifests:
  configmap_bin: true
  configmap_etc: true

  deployment_api: true
  deployment_conductor: true
  deployment_replica_cron: true
  deployment_worker: true

  # TODO: enable web UI:
  deployment_web: false
  deployment_web_proxy: false

  ingress_api: true
  job_bootstrap: true
  job_clean: true
  job_create_internal_tenant: true
  job_db_init: true
  job_rabbit_init: true
  job_db_sync: true
  job_db_drop: true
  job_ks_endpoints: true
  job_ks_service: true
  job_ks_user: true
  job_storage_init: true

  #pdb_api: true
  #pod_rally_test: true
  #pvc_backup: true
  #network_policy: false

  secret_db: true
  secret_ingress_tls: true
  secret_keystone: true
  secret_rabbitmq: true

  service_api: true
  service_ingress_api: true

########### auto-generated by `helm create`:
# replicaCount: 1

#image:
#  repository: nginx
#  pullPolicy: IfNotPresent

#imagePullSecrets: []
#nameOverride: ""
#fullnameOverride: ""

#serviceAccount:
#  # Specifies whether a service account should be created
#  create: true
#  # Annotations to add to the service account
#  annotations: {}
#  # The name of the service account to use.
#  # If not set and create is true, a name is generated using the fullname template
#  name:

#podSecurityContext: {}
#  # fsGroup: 2000

#securityContext: {}
#  # capabilities:
#  #   drop:
#  #   - ALL
#  # readOnlyRootFilesystem: true
#  # runAsNonRoot: true
#  # runAsUser: 1000

#service:
#  type: ClusterIP
#  port: 80

#ingress:
#  enabled: false
#  annotations: {}
#    # kubernetes.io/ingress.class: nginx
#    # kubernetes.io/tls-acme: "true"
#  hosts:
#    - host: chart-example.local
#      paths: []
#  tls: []
#  #  - secretName: chart-example-tls
#  #    hosts:
#  #      - chart-example.local

#resources: {}
#  # We usually recommend not to specify default resources and to leave this as a conscious
#  # choice for the user. This also increases chances charts run on environments with little
#  # resources, such as Minikube. If you do want to specify resources, uncomment the following
#  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
#  # limits:
#  #   cpu: 100m
#  #   memory: 128Mi
#  # requests:
#  #   cpu: 100m
#  #   memory: 128Mi

#nodeSelector: {}

#tolerations: []

#affinity: {}
